{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35595f8e",
   "metadata": {},
   "source": [
    "# This is still a work in progress and a rough template. Need to add hypernet to notebook still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b068a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import synecdoche as hn\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10  # MNIST has 10 classes (hand-written digits).\n",
    "\n",
    "class Batch(NamedTuple):\n",
    "    image: np.ndarray  # [B, H, W, 1]\n",
    "    label: np.ndarray  # [B]\n",
    "\n",
    "\n",
    "class TrainingState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    avg_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "\n",
    "\n",
    "def net_fn(images: jax.Array) -> jax.Array:\n",
    "    \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
    "    x = images.astype(jnp.float32) / 255.\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(300), jax.nn.relu,\n",
    "        hk.Linear(100), jax.nn.relu,\n",
    "        hk.Linear(NUM_CLASSES),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "        split: str,\n",
    "        *,\n",
    "        shuffle: bool,\n",
    "        batch_size: int,\n",
    "    ) -> Iterator[Batch]:\n",
    "    \"\"\"Loads the MNIST dataset.\"\"\"\n",
    "    ds, ds_info = tfds.load(\"mnist:3.*.*\", split=split, with_info=True)\n",
    "    ds.cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(ds_info.splits[split].num_examples, seed=0)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(lambda x: Batch(**x))\n",
    "    return iter(tfds.as_numpy(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec59914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params: hk.Params, batch: Batch) -> jax.Array:\n",
    "    \"\"\"Cross-entropy classification loss, regularised by L2 weight decay.\"\"\"\n",
    "    batch_size, *_ = batch.image.shape\n",
    "    logits = network.apply(params, batch.image)\n",
    "    labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
    "    l2_regulariser = 0.5 * sum(\n",
    "        jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "    log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "    return -log_likelihood / batch_size + 1e-4 * l2_regulariser\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params: hk.Params, batch: Batch) -> jax.Array:\n",
    "    \"\"\"Evaluation metric (classification accuracy).\"\"\"\n",
    "    logits = network.apply(params, batch.image)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(predictions == batch.label)\n",
    "\n",
    "@jax.jit\n",
    "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "    grads = jax.grad(loss)(state.params, batch)\n",
    "    updates, opt_state = optimiser.update(grads, state.opt_state)\n",
    "    params = optax.apply_updates(state.params, updates)\n",
    "    # Compute avg_params, the exponential moving average of the \"live\" params.\n",
    "    # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
    "    avg_params = optax.incremental_update(\n",
    "        params, state.avg_params, step_size=0.001)\n",
    "    return TrainingState(params, avg_params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b75fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make the network and optimiser.\n",
    "network = hk.without_apply_rng(hk.transform(net_fn))\n",
    "optimiser = optax.adam(1e-3)\n",
    "\n",
    "\n",
    "# Make datasets.\n",
    "train_dataset = load_dataset(\"train\", shuffle=True, batch_size=1_000)\n",
    "eval_datasets = {\n",
    "    split: load_dataset(split, shuffle=False, batch_size=10_000)\n",
    "    for split in (\"train\", \"test\")\n",
    "}\n",
    "# Initialise network and optimiser; note we draw an input to get shapes.\n",
    "initial_params = network.init(\n",
    "    jax.random.PRNGKey(seed=0), next(train_dataset).image)\n",
    "initial_opt_state = optimiser.init(initial_params)\n",
    "state = TrainingState(initial_params, initial_params, initial_opt_state)\n",
    "# Training & evaluation loop.\n",
    "for step in range(3001):\n",
    "    if step % 100 == 0:\n",
    "    # Periodically evaluate classification accuracy on train & test sets.\n",
    "    # Note that each evaluation is only on a (large) batch.\n",
    "    for split, dataset in eval_datasets.items():\n",
    "        accuracy = np.array(evaluate(state.avg_params, next(dataset))).item()\n",
    "        print({\"step\": step, \"split\": split, \"accuracy\": f\"{accuracy:.3f}\"})\n",
    "    # Do SGD on a batch of training examples.\n",
    "    state = update(state, next(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec97a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make the network and optimiser.\n",
    "network = hk.without_apply_rng(hk.transform(net_fn))\n",
    "optimiser = optax.adam(1e-3)\n",
    "\n",
    "\n",
    "# Make datasets.\n",
    "train_dataset = load_dataset(\"train\", shuffle=True, batch_size=1_000)\n",
    "eval_datasets = {\n",
    "    split: load_dataset(split, shuffle=False, batch_size=10_000)\n",
    "    for split in (\"train\", \"test\")\n",
    "}\n",
    "# Initialise network and optimiser; note we draw an input to get shapes.\n",
    "initial_params = network.init(\n",
    "    jax.random.PRNGKey(seed=0), next(train_dataset).image)\n",
    "initial_opt_state = optimiser.init(initial_params)\n",
    "state = TrainingState(initial_params, initial_params, initial_opt_state)\n",
    "# Training & evaluation loop.\n",
    "for step in range(3001):\n",
    "    if step % 100 == 0:\n",
    "    # Periodically evaluate classification accuracy on train & test sets.\n",
    "    # Note that each evaluation is only on a (large) batch.\n",
    "    for split, dataset in eval_datasets.items():\n",
    "        accuracy = np.array(evaluate(state.avg_params, next(dataset))).item()\n",
    "        print({\"step\": step, \"split\": split, \"accuracy\": f\"{accuracy:.3f}\"})\n",
    "    # Do SGD on a batch of training examples.\n",
    "    state = update(state, next(train_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
